---
title: "Fourth workshop module: Using Amazon Rekognition Custom Labels from R"
output: html_notebook
---

## Introduction

> **Info**
> 
> At the time of writing this article ( 20Dec20), Amazon Rekognition Custom Labels is available in 4 regions
> us-east-1 (N. Virginia), us-east-2 (Ohio), us-west-2 (Oregon), and eu-west-1 (Ireland).
> Please make sure to update the `AWS_REGION` entry in the `.Renviron` file in case you configured another AWS region. 
>
> Amazon Rekognition Custom Labels is currently also part of the AWS Free Tier which means you can get started for free:
> The [service-specific Free Tier](https://aws.amazon.com/rekognition/pricing/) lasts 3 months and includes 10 free training hours per month and 4 free inference 
> hours per month.

You can use Rekogntion Custom Labels to train your own ML models that performs image classification (image level predictions) or object detection (object/bounding box level predictions).

In this workshop module we will train an object detection model to detect the Swoosh, Nike's famous logo that Carolyn Davidson designed in 1971.

<p align="center">
```{r echo=FALSE, out.width = "40%"}
knitr::include_graphics("./images/nike_logo.png")
```


The process of using Rekognition Custom Labels to train, evaluate, deploy and use image classification or object detection models is the same and consists of the following steps:

* Step 0: Preprocess your image data
* Step 1: Create a S3 Custom Labels default bucket in your region
* Step 2: Create your project
* Step 3: Upload your dataset to S3
* Step 4: Create a Rekognition Custom Labels dataset
* Step 5: Train your model
* Step 6: Evaluate the training results
* Step 7: Deploy your model
* Step 8: Make real-time predictions for new data
* Step 9: Stop your model

We will follow the steps described above to build our Swoosh detection model. Not all of the steps are supported by the Rekognition API. For these steps we will switch over to the Amazon Rekognition Custom Labels Console.  


## Step 0: Preprocess your image data

> **Info**
>
> The minimum and the maximum image dimension of images used for training jobs and for inference is 
> 64 pixels x 64 pixels and 4096 pixels x 4096 pixels respectively. Make sure to scale your images 
> accordingly before you use Amazon Rekognition Custom Labels. 
>
> Additional requirements like supported image file formats, maximum image size, maximum number 
> of labels per image are described [in the official documentation here](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/limits.html).

We already did the heavy-lifting for you and pre-processed the images you will use to train the Swoosh detector using the purrr-EBImage-recipe below. You can use the recipe in your future projects if you need to scale down your images below the 4096 pixels threshold. 

```{r eval=FALSE}
library("EBImage")
library(purrr)

source_folder <- "./[folder_of_images_to_be_scaled]"
target_folder <- "./[folder_to_save_scaled_images/]"

walk(file_names, function(x) {
  img <- readImage(paste0(source_folder, "/", x))
  img_dim <- dim(img)
  index <- which.max(img_dim)
  
  if(img_dim[index] > 4096) {
    scale_fct <- 4096 / img_dim[index]
    img <- resize(img, w = img_dim[1] * scale_fct, h = img_dim[2] * scale_fct)
  }
  writeImage(img, paste0(target_folder, "/", x), quality = 95)
})
```


We collected x images containing the Nike Swoosh logo form [pexels.com](https://www.pexels.com/), scaled down the images using the code recipe above and then uploaded the preprocessed dataset to Kaggle which you can download from here. We used x images to train the Swoosh detector and x images as an hold-out test set for making real-time predictions. 

## Load the necessary libraries 

```{r message=FALSE }
library(paws)
library(purrr)
library(readr)
library(tibble)
library(jsonlite)
library(magick)
```


## Step 1: Create S3 Custom Labels bucket

Creating the S3 Custom Labels bucket is an one-time step per region in which you like to use Amazon Rekognition Custom Labels which you don't need to repeat afterwards. 

In the AWS Console select **Amazon Rekognition** underneath services and then select **Use Custom Labels** in the left sidebar. Click on **Get started** in the middle of the Amazon Rekognition Custom Labels Console and then on **Create S3 bucket** to create your Rekognition Custom Labels default bucket in your region:

![](images/00_create_default_bucket.PNG)

You can safely ignore the prompt in the console to create your first Custom Labels project. We will do this in the next step via the API.

Next, we create an S3 client to retrieve the name of our S3 Rekognition Custom Labels default bucket which we will need later: 

```{r}
s3 <- s3()
region <- Sys.getenv("AWS_REGION")

custom_labels_bucket <- s3$list_buckets() %>% 
  .[["Buckets"]] %>% 
  map_chr("Name") %>% 
  keep(function(x) startsWith(x, paste0("custom-labels-console-", region)))

custom_labels_bucket
```

## Step 2: Create your project

The primary purpose of Rekognition Custom Labels projects is to manage the lifecycle of your ML models. A trained model always belongs to one project. A project just serves as an umbrella under which you train, deploy and manage one or more ML models. 


Within Amazon Rekognition Custom Labels, you use projects to manage the models that you create. A project manages the input images and labels, datasets, model training, model evaluation, and running the model

We will create a Rekognition client and create our first Custom Labels project:

```{r}
rek <- rekognition()

project_arn <- rek$create_project("got_image_classification")
```

`create_project()` returns the project's Amazon Resource Name (ARN). We will store it in a separate variable which we will need later when defining the training job for our image classification model. 


Alternatively, you can also use the following code snipped to retrieve the entire list of your Rekognition Custom Labels projects and to select the project of your choice:

```{r eval=FALSE}
rek$describe_projects() %>% 
  pluck("ProjectDescriptions") %>% 
  map_chr("ProjectArn")
```


## Step 3: Upload your dataset


Now it is time to upload the image dataset to our S3 Custom Labels default bucket that we will use to train our Game of Thrones image classification model. But wait! We don't have these images yet. 

We will use the [Game of thrones character classification dataset](https://www.kaggle.com/kushagrakinjawadekar/game-of-thrones-character-classification) from Kaggle. Just press the **Download (115MB)** button on the page to download the dataset to your machine. Once the download is complete, please unzip *archive.zip*. 

Now we will create two new folders in our S3 Custom Labels default bucket. 

```{r}
s3$put_object(Bucket = custom_labels_bucket, Key = "/assets/got_characters_kaggle/", Body = "")

```

Underneath `/assets` we will organize our raw data sets and the outputs of our training runs

we will store all raw data sets we will use in our Custom Labels projects in separate folders. We will now upload.




s3 = boto3.client('s3')
bucket_name = "aniketbucketpython"
directory_name = "aniket/bucket/python" #it's name of your folders
s3.put_object(Bucket=bucket_name, Key=(directory_name+'/'))


Next, we navigate to the Amazon S3 Console and create 




## Step 4: Create a Rekognition Custom Labels dataset

Now we need to register the data set we uploaded to the S3 Custom Labels bucket in Rekognition Custom Labels. 
This step is necessary so that we can create a project/training job in Rekognition Custom Labels that will reference the registered data set. 

We do this to create the manifest file which we need to reference in the next step when starting a training job. 
The manifest file includes the correct labels of our images. 


This step is not yet supported via the official API and thus nor via paws. 

Rek CL datasets cannot be deleted. It seems you need to delete the underlying referenced S3 bucket content to get rid of the dataset in the Rek console. 
solution: delete datasets/ and index/ folder in the S3 Custom Labels folder which will automatically remove dataset in Rek console. 


## Step 7: Deploy your model

```{r}

model_arn <- training_results %>% 
  pluck("ProjectVersionDescriptions", 1, "ProjectVersionArn")


rek$start_project_version(ProjectVersionArn = model_arn, 
                          MinInferenceUnits = 1 )

```

### dec12

```{r}
project_arns <- rek$describe_projects() %>% 
  pluck("ProjectDescriptions") %>% 
  map_chr("ProjectArn") 

swoosh_project <- project_arns[grepl("swoosh_detection3", project_arns)]

#rek$describe_project_versions(swoosh_project)

model_arn <- rek$describe_project_versions(swoosch_project) %>% 
  pluck("ProjectVersionDescriptions" , 1, "ProjectVersionArn")
  
```

```{r}
rek$start_project_version(ProjectVersionArn = model_arn, MinInferenceUnits = 1)
```

```{r}
#rek$describe_project_versions(project_arn, version_name) %>% 
#  pluck("ProjectVersionDescriptions", 1, "Status")

rek$describe_project_versions(swoosh_project, "swoosh_detection3.2020-11-29T20.53.55") %>% 
  pluck("ProjectVersionDescriptions", 1, "Status")
```


## Step 8: Make real-time predictions for new data


```{r}
file_names <- list.files("./images/inference/")
path_to_file <- "./images/inference/"
```

### Image 1

```{r}
file <- paste0(path_to_file, file_names[1])
image <- read_file_raw(file)

resp <- rek$detect_custom_labels(ProjectVersionArn = model_arn,
                                 Image = list(
                                   Bytes = image
                                 )
)

# Parse the result
names <- resp$CustomLabels %>% 
  map_chr("Name")
confidence <- resp$CustomLabels %>% 
  map_dbl("Confidence")

tibble(names, confidence)
```


```{r fig.align="center", out.width = "60%"}
# Convert raw image into a magick object
magick_image <- image_read(file)
image_attr <- image_info(magick_image)

# Extract bounding box information of detected object(s) from the response
bounding_box <- resp$CustomLabels[[1]]$Geometry$BoundingBox

# Calculate bounding box properties
width <- bounding_box$Width * image_attr$width 
height <- bounding_box$Height * image_attr$height
left <- bounding_box$Left * image_attr$width 
top <- bounding_box$Top * image_attr$height

# Add bounding box to image
image <- magick_image %>% 
  image_draw()
rect(left, top, left + width, top + height, border = "red", lty = "dashed", lwd = 15)
dev.off()
```

Image 2

```{r}
file <- paste0(path_to_file, file_names[2])
image <- read_file_raw(file)

resp <- rek$detect_custom_labels(ProjectVersionArn = model_arn,
                                 Image = list(
                                   Bytes = image
                                 )
)

# Parse the result
names <- resp$CustomLabels %>% 
  map_chr("Name")
confidence <- resp$CustomLabels %>% 
  map_dbl("Confidence")

tibble(names, confidence)
```

```{r fig.align="center", out.width = "60%"}
# Convert raw image into a magick object
magick_image <- image_read(file)
image_attr <- image_info(magick_image)

# Extract bounding box information of detected object(s) from the response
bounding_box <- resp$CustomLabels[[1]]$Geometry$BoundingBox

# Calculate bounding box properties
width <- bounding_box$Width * image_attr$width 
height <- bounding_box$Height * image_attr$height
left <- bounding_box$Left * image_attr$width 
top <- bounding_box$Top * image_attr$height

# Add bounding box to image
image <- magick_image %>% 
  image_draw()
rect(left, top, left + width, top + height, border = "red", lty = "dashed", lwd = 15)
dev.off()
```
### Image 3

```{r}
file <- paste0(path_to_file, file_names[3])
image <- read_file_raw(file)

resp <- rek$detect_custom_labels(ProjectVersionArn = model_arn,
                                 Image = list(
                                   Bytes = image
                                 )
)

# Parse the result
names <- resp$CustomLabels %>% 
  map_chr("Name")
confidence <- resp$CustomLabels %>% 
  map_dbl("Confidence")

tibble(names, confidence)
```

### Image 4

```{r}
file <- paste0(path_to_file, file_names[4])
image <- read_file_raw(file)

resp <- rek$detect_custom_labels(ProjectVersionArn = model_arn,
                                 Image = list(
                                   Bytes = image
                                 )
)

# Parse the result
names <- resp$CustomLabels %>% 
  map_chr("Name")
confidence <- resp$CustomLabels %>% 
  map_dbl("Confidence")

tibble(names, confidence)
```

Recommend to use this pure recipe


```{r fig.align="center", out.width = "60%"}
# Convert raw image into a magick object
magick_image <- image_read(file)
image_attr <- image_info(magick_image)

# Extract bounding box information of detected object(s) from the response
bounding_boxes <- resp$CustomLabels

# Calculate bounding box properties
boxes <- map_dfr(bounding_boxes, function(x) {
  bounding_box <- x[["Geometry"]][["BoundingBox"]]
  width <- bounding_box$Width * image_attr$width 
  height <- bounding_box$Height * image_attr$height
  left <- bounding_box$Left * image_attr$width 
  top <- bounding_box$Top * image_attr$height
  cords <- c(width = width, height = height, left = left, top = top)
  cords
})

# Add bounding box(es) to image
image <- magick_image %>% 
  image_draw()
rect(boxes$left, boxes$top, boxes$left + boxes$width, boxes$top + boxes$height, border = "red", lty = "dashed", lwd = 15)
dev.off()
```


### Image 5

```{r}
file <- paste0(path_to_file, file_names[5])
image <- read_file_raw(file)

resp <- rek$detect_custom_labels(ProjectVersionArn = model_arn,
                                 Image = list(
                                   Bytes = image
                                 )
)

# Parse the result
names <- resp$CustomLabels %>% 
  map_chr("Name")
confidence <- resp$CustomLabels %>% 
  map_dbl("Confidence")

tibble(names, confidence)
```
```{r fig.align="center", out.width = "60%"}
# Convert raw image into a magick object
magick_image <- image_read(file)
image_attr <- image_info(magick_image)

# Extract bounding box information of detected object(s) from the response
bounding_boxes <- resp$CustomLabels

# Calculate bounding box properties
boxes <- map_dfr(bounding_boxes, function(x) {
  bounding_box <- x[["Geometry"]][["BoundingBox"]]
  width <- bounding_box$Width * image_attr$width 
  height <- bounding_box$Height * image_attr$height
  left <- bounding_box$Left * image_attr$width 
  top <- bounding_box$Top * image_attr$height
  cords <- c(width = width, height = height, left = left, top = top)
  cords
})

# Add bounding box(es) to image
image <- magick_image %>% 
  image_draw()
rect(boxes$left, boxes$top, boxes$left + boxes$width, boxes$top + boxes$height, border = "red", lty = "dashed", lwd = 15)
dev.off()
```

## Step 9: Stop your model

```{r}
resp <- rek$stop_project_version(model_arn)
```

## Summary 




